---
title: "What's Really Blocking AI Agents from Production? Insights from 36 Expert Interviews"
summary: 'Research spanning 36 expert interviews, 5 industry conferences, and 3 functional prototypes reveals that AI agent deployment is fundamentally an engineering problem, not an AI problem—with models contributing only 30-40% to success while framework architecture, integration, and business case clarity drive the remaining 60-70%.'
publishedAt: '2025-12-10'
tags: ['AI Agents', 'Research', 'Enterprise AI', 'Production', 'Stanford GSB']
featured: true
author: 'Fernando Torres'
image: '/images/blog/ai-agents-research-cover.png'
---

{/_ ==========================================================================
ANCHOR POST OUTLINE
Publication 1: Research Overview
Target length: 2,500-3,000 words
========================================================================== _/}

{/_ SECTION 1: HOOK
The demo-production gap problem - 90% fail statistic
Target: 2-3 punchy sentences that grab attention
========================================================================== _/}

## Hook Section

[OUTLINE - 2-3 sentences]

- Open with the striking 90% pilot failure statistic (the practitioner/an AI agent orchestration company)
- Contrast: Demos impress, pilots proliferate, but production conversion hovers around 10%
- Tease the counterintuitive finding: The bottleneck is not AI intelligence
- Pattern interrupt: What if the problem was never about making models smarter?

Key statistics to incorporate:

- 90% of enterprise AI agent pilots never convert to production (the practitioner/an AI agent orchestration company)
- Demos work 70% of the time, which is enough to impress but systematically fails in production (the founder/an AI infrastructure company)

{/_ SECTION 2: RESEARCH METHODOLOGY
36 interviews, 5 conferences, 3 prototypes
Target: 150-200 words establishing credibility
========================================================================== _/}

## The Discovery: Research Methodology

[OUTLINE - 150-200 words]

### Research Scope

- **36 Expert Interviews**: Practitioners actively deploying agentic workflows
  - Enterprise AI platform providers (an AI agent orchestration company, an AI infrastructure company, an AI observability company)
  - Coding agent developers (an AI coding company)
  - Identity and security experts (a major enterprise identity company team member)
  - Vertical SaaS companies (an AI sales intelligence company, a CRM AI company, a workforce platform)
  - Framework companies (a multi-agent framework company, Wise Agents)
  - Infrastructure providers (an AI infrastructure company)
  - Foundation model companies (Anthropic)

- **5 Industry Conferences**: Leading AI agent events (Sept-Nov 2024)
  - Alibaba Qwen Conference
  - Production Agents Summit at Snowflake
  - an AI autonomous agent company Fireside
  - Project Nanda
  - "Why 95% of Agentic AI Projects Fail"

- **3 Functional Prototypes**: Hands-on validation
  - Shopping Agent (e-commerce automation)
  - Repo Patcher (code fixing agent with state machine architecture)
  - Good Agents (multi-agent orchestration)

### Research Timeline

- 10 weeks of systematic investigation (September-November 2024)
- Stanford GSB GSBGEN 390 Individual Research project

{/_ SECTION 3: KEY FINDING 1
30-40% model / 60-70% architecture insight
Target: 300-400 words with supporting quotes
========================================================================== _/}

## What We Found: Key Finding 1 - The 30-40% Model Revelation

Going into this research, I expected to hear that model capabilities were the primary bottleneck. The prevailing narrative suggested that if we could just make models smarter, more reliable, and more capable, production deployments would follow. I was wrong.

> "We found out actually model only maybe contributes 30 or 40% of the whole thing. And the framework, the whole system you build upon the model is much more important, you know, than the model itself."
> — an AI autonomous agent company Co-Founder, an AI autonomous agent company Fireside

This revelation from one of the most successful AI agent companies in production fundamentally reoriented my research. The model is not the differentiator. Framework architecture, orchestration design, and system integration are what determine whether an agent succeeds or fails in production environments. The 60-70% that actually matters includes context engineering, integration layers, evaluation infrastructure, and the orchestration logic that coordinates model calls.

Our theme frequency analysis validated this insight quantitatively. Model Capabilities appeared in only 62% of sources, making it the least prevalent of our six core themes. In stark contrast, System Integration appeared in 92% of sources, the highest frequency of any theme. The practitioners building production agents were not complaining about model intelligence. They were wrestling with how to connect systems, manage state, and coordinate complex workflows.

This inversion carries profound implications for competitive strategy. Moats will not come from privileged model access as foundation models commoditize. The defensible value lies in proprietary integration architectures, domain-specific orchestration patterns, and evaluation systems that compound learning over time. Companies waiting for GPT-5 to solve their deployment problems are investing in the wrong 30-40% of the solution.

{/_ SECTION 4: KEY FINDING 2
90% pilot failure - business case failure
Target: 300-400 words with supporting quotes
========================================================================== _/}

## Key Finding 2: Business Case Failure Precedes Technical Failure

The most counterintuitive finding from this research emerged in my very first interview. I had prepared a comprehensive list of technical questions about prompt engineering, context management, and model capabilities. Within the first ten minutes, the conversation took an unexpected turn that reshaped my entire research framework.

> "Most AI agent deployments fail due to undefined ROI calculations and lack of commercial mindset, not technical limitations."
> — the practitioner, an AI agent orchestration company

This was not an outlier perspective. The same practitioner revealed that 90% of all enterprise AI agent pilots never convert to production. They stall, not because the technology fails, but because no one defined success criteria that mattered to the business. Pilots proliferate because demos are impressive and stakeholders get excited. But when procurement asks for projected ROI or when executives need to justify ongoing infrastructure costs, the conversation collapses.

The pricing model confusion compounds this problem. Today, AI agent vendors offer seat-based pricing, token-based pricing, outcome-based pricing, and hybrid models, with no industry convergence in sight. Enterprises find themselves unable to model their usage or predict their costs. As one practitioner explained, they cannot calculate how many tokens a workflow will consume because agent behavior varies based on inputs, context, and the unpredictable nature of multi-step reasoning.

> "All of these pricing changes or type of pricing schemes is confusing to large enterprises. They don't know how many tokens they're going to use. They can't model their usage, they can't model their outcome."
> — an engineering leader at a major identity company, a major enterprise identity company

Our theme frequency analysis confirmed this finding was not isolated. Business Case and ROI emerged as a top emergent theme with a perfect 5.0 relevance score across five sources. Enterprise Blockers and Governance appeared in 81% of all sources analyzed. The pattern was clear: before technical viability even becomes relevant, economic viability and organizational readiness must be established. The companies succeeding in production are those that defined their success metrics and ROI calculations before writing their first line of agent code.

{/_ SECTION 5: KEY FINDING 3
Framework abandonment pattern 80-90%
Target: 300-400 words with supporting quotes
========================================================================== _/}

## Key Finding 3: The Framework Abandonment Pattern

Perhaps no finding better captures the gap between industry narrative and production reality than the framework abandonment pattern. a popular AI agent framework has achieved unicorn status with a billion-dollar valuation and massive developer adoption. It has become synonymous with AI agent development for many practitioners entering the space. Yet our research uncovered a striking counter-narrative from those actually shipping agents to production.

> "Every company we've talked to started with a popular AI agent framework as a framework to build AI agents. But once they start going into customers and into production, they realize it's full of bloat. Like, it has a lot of unnecessary things. They end up ditching that solution, and they build their own. This has been like 80, 90% of the clients we've talked to."
> — Cynthia, Wise Agents

The 80-90% abandonment rate is not about ideology or preferences. It reflects hard performance realities. Teams building custom frameworks report achieving 3-4x faster performance than a popular AI agent framework implementations, according to a practitioner from a CRM AI company. The abstractions that accelerate prototyping become obstacles when latency, cost, and control matter at production scale.

Our own prototype work validated this pattern firsthand. When building the Shopping Agent for e-commerce automation, we were forced to switch from LangGraph mid-development due to what our team documented as extensive bloat and complexity. The framework that promised to accelerate development instead became the primary source of friction. We experienced exactly what interview subjects had described.

This finding has important implications for the framework ecosystem. Unlike frontend frameworks where React eventually consolidated the market, agent frameworks may remain permanently fragmented. The requirements for experimentation diverge too sharply from production needs. Frameworks optimized for rapid prototyping will continue adding features that serve that use case, while production teams will continue building bespoke solutions optimized for their specific performance and control requirements.

## Why This Matters

The implications of these findings extend far beyond academic interest. For anyone working with AI agents—whether you are building them, buying them, or betting on them—the shift from "AI problem" to "engineering problem" fundamentally changes where to focus resources and attention.

**Engineering teams** should recognize that their competitive advantage lies not in model selection or prompt engineering, but in the surrounding infrastructure. The 60-70% that determines success—framework architecture, integration layers, evaluation systems, and context engineering—represents territory where traditional software engineering expertise becomes the differentiator. Teams with strong systems design capabilities have an unexpected advantage in the AI agent era.

**Enterprise leaders** face a sobering reality: 90% of pilots fail, and most failures stem from undefined business cases rather than technical limitations. Before authorizing another AI agent initiative, demand clarity on ROI calculations, understand that 40-50% of deployment time will be consumed by integration work, and budget for the near-certainty of framework migration as prototypes evolve into production systems.

**Investors and product strategists** should look beyond model capability announcements. The companies poised to capture value are those solving the unsexy problems: governance frameworks, cost predictability, evaluation infrastructure, and enterprise identity. As models commoditize—and the research suggests current capabilities are already "good enough" for many use cases—sustainable differentiation will come from the engineering layers that make agents reliable, observable, and economically viable.

The research also signals a timing consideration. Multiple sources suggested that current harnesses and frameworks may become obsolete as the field matures. Early movers who over-invest in today's tooling risk building on shifting foundations. The winners will be those who invest in adaptable architectures rather than rigid framework commitments.

## What You Can Do

If this research resonates with your experience—or contradicts it—here are concrete steps to apply these findings:

- **Focus engineering resources on framework architecture, not just model selection.** The 60-70% that matters most is the orchestration layer, integration architecture, and evaluation infrastructure you build around the model. Stop waiting for GPT-5 to solve deployment problems.

- **Define your business case and ROI before technical implementation.** 90% of pilots fail due to undefined success criteria. Before writing agent code, establish how you will measure value and what cost structure is sustainable.

- **Evaluate frameworks for production characteristics, not prototyping speed.** The framework that gets you to a demo fastest may be the one you abandon when latency and control matter. Budget for migration.

- **Implement component-level evaluation over end-to-end testing.** As one practitioner noted, they "almost never evaluate end-to-end because it is pointless." Test the deterministic components—especially retrieval and first-step accuracy—rather than the full probabilistic pipeline.

- **Plan for 40-50% of deployment time on integration work.** System integration appeared in 92% of our sources for a reason. Enterprise systems that "never talked to each other" will consume the majority of your implementation effort.

- **Consider handoff rate as your primary success metric.** The real question is whether your agent actually reduces the work passed back to humans. Accuracy, latency, and token efficiency matter less if tasks still require human completion.

## Coming Next: The Deep Dive Series

This anchor post summarizes the headline findings from ten weeks of research. But each theme deserves deeper exploration, and the coming series will unpack the nuances, contradictions, and practical implications that emerged from our interviews and prototype work.

The **theme deep dives** will examine each of the six core findings in detail. We will explore why system integration dominates 92% of practitioner concerns, unpack the counterintuitive 40% context utilization rule, and tell the full story of framework abandonment through the voices of practitioners who lived it. We will examine why 70% demo accuracy creates false expectations, how business case failure precedes technical failure, and why coding agents succeed where other domains struggle.

Beyond the core themes, we will share **emergent insights** that surprised us during the research—including the dual memory architecture distinction, the MCP 25-tool accuracy cliff, and the evaluation gap that leaves seven YC companies without adoption.

The series will also feature **practitioner perspectives** from named sources who agreed to share their experiences, **prototype case studies** from our hands-on building work, and **conference insights** from the events that shaped our understanding. Each post will stand alone while contributing to a comprehensive picture of production AI agent deployment.

## The Bottom Line

After 36 interviews, 5 conferences, and 3 prototypes, the pattern is unmistakable: production AI agents are an engineering problem, not an AI problem. The companies that will win are not those chasing the next model release but those building the frameworks, integration layers, and evaluation systems that constitute the 60-70% that actually determines success. Stop waiting for smarter models. Start building better systems.

---

**Research Context**

_This research was conducted as part of Stanford GSB GSBGEN 390 Individual Research during Autumn 2025. The study comprises 36 expert interviews with practitioners across enterprise AI platforms, coding agents, vertical SaaS, framework companies, and infrastructure providers; 5 industry conferences including the Production Agents Summit and an AI autonomous agent company Fireside; and 3 functional prototypes built to validate interview findings. All statistics and quotes are traceable to source extractions._

---

_This post is part of my research series on AI Agent deployment. The series will explore each theme in depth through dedicated posts on system integration, context management, framework selection, probabilistic reliability, enterprise blockers, and model capabilities—plus emergent insights, practitioner perspectives, and prototype case studies._

_Have thoughts on AI agent deployment? I would love to hear from you—whether these findings match your experience or contradict it entirely._

**Connect with me:**

- [LinkedIn](https://www.linkedin.com/in/fernandotn/)
- [Email](mailto:fertorresnavarrete@gmail.com)
