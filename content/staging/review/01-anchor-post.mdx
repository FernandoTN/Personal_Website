---
title: "What's Really Blocking AI Agents from Production? Insights from 36 Expert Interviews"
summary: 'Research spanning 36 expert interviews, 5 industry conferences, and 3 functional prototypes reveals that AI agent deployment is fundamentally an engineering problem, not an AI problem—with models contributing only 30-40% to success while framework architecture, integration, and business case clarity drive the remaining 60-70%.'
publishedAt: '2025-12-10'
tags: ['AI Agents', 'Research', 'Enterprise AI', 'Production', 'Stanford GSB']
featured: true
author: 'Fernando Torres'
image: '/images/blog/ai-agents-research-cover.png'
---

Ninety percent of enterprise AI agent pilots never make it to production. Not because the technology fails in demos—demos work brilliantly around 70% of the time, which is more than enough to impress stakeholders and generate excitement. The pilots proliferate, the budgets flow, and the teams mobilize. Yet when deployment day arrives, something breaks down. After spending ten weeks interviewing 36 practitioners who are building and deploying AI agents in production environments, I discovered something unexpected: the bottleneck is not AI intelligence. What if the problem was never about making models smarter?

## The Discovery

This research began with a straightforward hypothesis: AI agent deployment challenges stem primarily from model capabilities—context window limitations, reasoning failures, hallucinations. I expected to find engineering teams waiting for GPT-5 or the next breakthrough from Anthropic to unlock production readiness.

What I found inverted that assumption entirely.

Over ten weeks of systematic investigation as part of Stanford GSB's GSBGEN 390 Individual Research project, I conducted 36 expert interviews with practitioners actively deploying agentic workflows across the industry. These included enterprise AI platform providers like an AI agent orchestration company and an AI infrastructure company, coding agent developers at an AI coding company, identity and security experts from a major enterprise identity company, vertical SaaS companies including an AI sales intelligence company, a CRM AI company, and a workforce platform, framework companies like a multi-agent framework company and a consulting firm, infrastructure providers such as an AI infrastructure company, and researchers from foundation model companies.

I supplemented these interviews with observations from five industry conferences held between September and November 2024: the Alibaba Qwen Conference, Production Agents Summit at Snowflake, an AI autonomous agent company Fireside, Project Nanda, and a provocatively titled event called "Why 95% of Agentic AI Projects Fail." Finally, I built three functional prototypes with my hands on the keyboard: a Shopping Agent for e-commerce automation, Repo Patcher with state machine architecture for code fixing, and Good Agents for multi-agent orchestration.

The patterns that emerged from triangulating these sources fundamentally changed how I understand AI agent deployment.

## What We Found: Key Finding 1 - The 30-40% Model Revelation

Going into this research, I expected to hear that model capabilities were the primary bottleneck. The prevailing narrative suggested that if we could just make models smarter, more reliable, and more capable, production deployments would follow. I was wrong.

> "We found out actually model only maybe contributes 30 or 40% of the whole thing. And the framework, the whole system you build upon the model is much more important, you know, than the model itself."
> — an AI autonomous agent company Co-Founder, an AI autonomous agent company Fireside

This revelation from one of the most successful AI agent companies in production fundamentally reoriented my research. The model is not the differentiator. Framework architecture, orchestration design, and system integration are what determine whether an agent succeeds or fails in production environments. The 60-70% that actually matters includes context engineering, integration layers, evaluation infrastructure, and the orchestration logic that coordinates model calls.

Our theme frequency analysis validated this insight quantitatively. Model Capabilities appeared in only 62% of sources, making it the least prevalent of our six core themes. In stark contrast, System Integration appeared in 92% of sources, the highest frequency of any theme. The practitioners building production agents were not complaining about model intelligence. They were wrestling with how to connect systems, manage state, and coordinate complex workflows.

This inversion carries profound implications for competitive strategy. Moats will not come from privileged model access as foundation models commoditize. The defensible value lies in proprietary integration architectures, domain-specific orchestration patterns, and evaluation systems that compound learning over time. Companies waiting for GPT-5 to solve their deployment problems are investing in the wrong 30-40% of the solution.

## Key Finding 2: Business Case Failure Precedes Technical Failure

The most counterintuitive finding from this research emerged in my very first interview. I had prepared a comprehensive list of technical questions about prompt engineering, context management, and model capabilities. Within the first ten minutes, the conversation took an unexpected turn that reshaped my entire research framework.

> "Most AI agent deployments fail due to undefined ROI calculations and lack of commercial mindset, not technical limitations."
> — the practitioner, an AI agent orchestration company

This was not an outlier perspective. The same practitioner revealed that 90% of all enterprise AI agent pilots never convert to production. They stall, not because the technology fails, but because no one defined success criteria that mattered to the business. Pilots proliferate because demos are impressive and stakeholders get excited. But when procurement asks for projected ROI or when executives need to justify ongoing infrastructure costs, the conversation collapses.

The pricing model confusion compounds this problem. Today, AI agent vendors offer seat-based pricing, token-based pricing, outcome-based pricing, and hybrid models, with no industry convergence in sight. Enterprises find themselves unable to model their usage or predict their costs. As one practitioner explained, they cannot calculate how many tokens a workflow will consume because agent behavior varies based on inputs, context, and the unpredictable nature of multi-step reasoning.

> "All of these pricing changes or type of pricing schemes is confusing to large enterprises. They don't know how many tokens they're going to use. They can't model their usage, they can't model their outcome."
> — an engineering leader at a major identity company, a major enterprise identity company

Our theme frequency analysis confirmed this finding was not isolated. Business Case and ROI emerged as a top emergent theme with a perfect 5.0 relevance score across five sources. Enterprise Blockers and Governance appeared in 81% of all sources analyzed. The pattern was clear: before technical viability even becomes relevant, economic viability and organizational readiness must be established. The companies succeeding in production are those that defined their success metrics and ROI calculations before writing their first line of agent code.

## Key Finding 3: The Framework Abandonment Pattern

Perhaps no finding better captures the gap between industry narrative and production reality than the framework abandonment pattern. a popular AI agent framework has achieved unicorn status with a billion-dollar valuation and massive developer adoption. It has become synonymous with AI agent development for many practitioners entering the space. Yet our research uncovered a striking counter-narrative from those actually shipping agents to production.

> "Every company we've talked to started with a popular AI agent framework as a framework to build AI agents. But once they start going into customers and into production, they realize it's full of bloat. Like, it has a lot of unnecessary things. They end up ditching that solution, and they build their own. This has been like 80, 90% of the clients we've talked to."
> — Cynthia, a consulting firm

The 80-90% abandonment rate is not about ideology or preferences. It reflects hard performance realities. Teams building custom frameworks report achieving 3-4x faster performance than a popular AI agent framework implementations, according to a practitioner from a CRM AI company. The abstractions that accelerate prototyping become obstacles when latency, cost, and control matter at production scale.

Our own prototype work validated this pattern firsthand. When building the Shopping Agent for e-commerce automation, we were forced to switch from LangGraph mid-development due to what our team documented as extensive bloat and complexity. The framework that promised to accelerate development instead became the primary source of friction. We experienced exactly what interview subjects had described.

This finding has important implications for the framework ecosystem. Unlike frontend frameworks where React eventually consolidated the market, agent frameworks may remain permanently fragmented. The requirements for experimentation diverge too sharply from production needs. Frameworks optimized for rapid prototyping will continue adding features that serve that use case, while production teams will continue building bespoke solutions optimized for their specific performance and control requirements.

## Key Statistics Reference

Before diving into implications, here is a summary of the critical data points from this research:

| Finding                       | Statistic                                 | Source                                         |
| ----------------------------- | ----------------------------------------- | ---------------------------------------------- |
| System integration challenges | 92% of sources                            | Theme Frequency Analysis                       |
| Pilot failure rate            | 90% never reach production                | an enterprise AI deployment expert             |
| Framework abandonment         | 80-90% leave a popular AI agent framework | a consulting firm practitioner                 |
| Integration time allocation   | 40-50% of deployment                      | an enterprise AI deployment expert             |
| Model contribution to success | 30-40%                                    | an AI autonomous agent company Fireside        |
| Context utilization rule      | 40% max recommended                       | Production Agents Summit                       |
| MCP tool accuracy cliff       | 25 tools then 30% accuracy                | a practitioner at an AI infrastructure company |

These numbers recurred across interviews, conferences, and our own prototype work, suggesting they represent durable patterns rather than isolated experiences.

## Why This Matters

The implications of these findings extend far beyond academic interest. For anyone working with AI agents—whether you are building them, buying them, or betting on them—the shift from "AI problem" to "engineering problem" fundamentally changes where to focus resources and attention.

**Engineering teams** should recognize that their competitive advantage lies not in model selection or prompt engineering, but in the surrounding infrastructure. The 60-70% that determines success—framework architecture, integration layers, evaluation systems, and context engineering—represents territory where traditional software engineering expertise becomes the differentiator. Teams with strong systems design capabilities have an unexpected advantage in the AI agent era.

**Enterprise leaders** face a sobering reality: 90% of pilots fail, and most failures stem from undefined business cases rather than technical limitations. Before authorizing another AI agent initiative, demand clarity on ROI calculations, understand that 40-50% of deployment time will be consumed by integration work, and budget for the near-certainty of framework migration as prototypes evolve into production systems.

**Investors and product strategists** should look beyond model capability announcements. The companies poised to capture value are those solving the unsexy problems: governance frameworks, cost predictability, evaluation infrastructure, and enterprise identity. As models commoditize—and the research suggests current capabilities are already "good enough" for many use cases—sustainable differentiation will come from the engineering layers that make agents reliable, observable, and economically viable.

The research also signals a timing consideration. Multiple sources suggested that current harnesses and frameworks may become obsolete as the field matures. Early movers who over-invest in today's tooling risk building on shifting foundations. The winners will be those who invest in adaptable architectures rather than rigid framework commitments.

## What You Can Do

If this research resonates with your experience—or contradicts it—here are concrete steps to apply these findings:

- **Focus engineering resources on framework architecture, not just model selection.** The 60-70% that matters most is the orchestration layer, integration architecture, and evaluation infrastructure you build around the model. Stop waiting for GPT-5 to solve deployment problems.

- **Define your business case and ROI before technical implementation.** 90% of pilots fail due to undefined success criteria. Before writing agent code, establish how you will measure value and what cost structure is sustainable.

- **Evaluate frameworks for production characteristics, not prototyping speed.** The framework that gets you to a demo fastest may be the one you abandon when latency and control matter. Budget for migration.

- **Implement component-level evaluation over end-to-end testing.** As one practitioner noted, they "almost never evaluate end-to-end because it is pointless." Test the deterministic components—especially retrieval and first-step accuracy—rather than the full probabilistic pipeline.

- **Plan for 40-50% of deployment time on integration work.** System integration appeared in 92% of our sources for a reason. Enterprise systems that "never talked to each other" will consume the majority of your implementation effort.

- **Consider handoff rate as your primary success metric.** The real question is whether your agent actually reduces the work passed back to humans. Accuracy, latency, and token efficiency matter less if tasks still require human completion.

## Coming Next: The Deep Dive Series

This anchor post summarizes the headline findings from ten weeks of research. But each theme deserves deeper exploration, and the coming series will unpack the nuances, contradictions, and practical implications that emerged from our interviews and prototype work.

The **theme deep dives** will examine each of the six core findings in detail. We will explore why system integration dominates 92% of practitioner concerns, unpack the counterintuitive 40% context utilization rule, and tell the full story of framework abandonment through the voices of practitioners who lived it. We will examine why 70% demo accuracy creates false expectations, how business case failure precedes technical failure, and why coding agents succeed where other domains struggle.

Beyond the core themes, we will share **emergent insights** that surprised us during the research—including the dual memory architecture distinction, the MCP 25-tool accuracy cliff, and the evaluation gap that leaves seven YC companies without adoption.

The series will also feature **practitioner perspectives** from named sources who agreed to share their experiences, **prototype case studies** from our hands-on building work, and **conference insights** from the events that shaped our understanding. Each post will stand alone while contributing to a comprehensive picture of production AI agent deployment.

## Additional Findings: The Probabilistic Nature Problem

Beyond the three headline findings, our research uncovered a pervasive challenge that underlies many deployment failures: the mismatch between deterministic expectations and probabilistic reality.

> "Most people that have built and deployed technology over the last, say, 30 years aren't used to probabilistic stochastic systems. They're used to deterministic things. So the expectation is if I see it working once, I expect it to work reliably from that point on."
> — a founder at an AI infrastructure company, an AI infrastructure company

This conditioning creates what one practitioner called "the doom loop"—teams fix one failure scenario, only to find that their fix breaks previously working cases. Without systematic evaluation infrastructure and an understanding of probabilistic behavior, teams spiral into endless prompt tweaking rather than architectural improvements.

The 70% demo reliability threshold is particularly insidious. A demo that works 70% of the time is impressive enough to generate stakeholder buy-in and budget allocation. But that same 70% reliability systematically fails in production, where users expect consistency and where edge cases multiply with scale.

The research also revealed a surprising constraint on context window usage. Despite vendors racing to expand context windows to hundreds of thousands of tokens, the Production Agents Summit revealed that practical utilization should stay below 40%. Beyond this threshold, quality degrades regardless of total window size. The answer to context challenges is not larger windows but better context engineering: compression, summarization, just-in-time retrieval, and architectural patterns that keep active context focused and relevant.

Finally, MCP (Model Context Protocol), often positioned as the solution to integration challenges, carries its own limitations. When tools exceed 25 integrations, accuracy drops to 30% according to practitioner experience at an AI infrastructure company. The protocol that promises universal connectivity becomes a source of confusion for agents navigating too many options.

## The Bottom Line

After 36 interviews, 5 conferences, and 3 prototypes, the pattern is unmistakable: production AI agents are an engineering problem, not an AI problem. The companies that will win are not those chasing the next model release but those building the frameworks, integration layers, and evaluation systems that constitute the 60-70% that actually determines success. Stop waiting for smarter models. Start building better systems.

---

**Research Context**

_This research was conducted as part of Stanford GSB GSBGEN 390 Individual Research during Autumn 2025. The study comprises 36 expert interviews with practitioners across enterprise AI platforms, coding agents, vertical SaaS, framework companies, and infrastructure providers; 5 industry conferences including the Production Agents Summit and an AI autonomous agent company Fireside; and 3 functional prototypes built to validate interview findings. All statistics and quotes are traceable to source extractions._

---

_This post is part of my research series on AI Agent deployment. The series will explore each theme in depth through dedicated posts on system integration, context management, framework selection, probabilistic reliability, enterprise blockers, and model capabilities—plus emergent insights, practitioner perspectives, and prototype case studies._

_Have thoughts on AI agent deployment? I would love to hear from you—whether these findings match your experience or contradict it entirely._

**Connect with me:**

- [LinkedIn](https://www.linkedin.com/in/fernandotn/)
- [Email](mailto:fertorresnavarrete@gmail.com)
