# Infographic Specification: Production Reality Metrics

## Generated Image

![Infographic Metrics](generated-images/infographics/infographic-metrics.png)

**File Location:** `generated-images/infographics/infographic-metrics.png`

---

## Title

**"The Numbers Behind Production AI Agents"**

Subtitle: _What 36 Expert Interviews Revealed About Deployment Reality_

---

## Data Points

### Primary Metrics (Hero Numbers)

| Metric                                    | Visual Treatment                              | Source Frequency |
| ----------------------------------------- | --------------------------------------------- | ---------------- |
| **40-50% deployment time on integration** | Large circular gauge showing time allocation  | 15/26 sources    |
| **40% context window utilization rule**   | Memory bar at 40% fill with warning indicator | 8/26 sources     |
| **25-tool MCP accuracy threshold**        | Tool grid showing dropoff point               | 6/26 sources     |

### Supporting Metrics (Secondary Row)

| Metric                                        | Visual Treatment                        | Source Frequency |
| --------------------------------------------- | --------------------------------------- | ---------------- |
| **Component-level eval > End-to-end testing** | Split comparison showing component wins | Emergent theme   |
| **Handoff rate as success metric**            | Human-agent handoff icon with checkmark | Emergent theme   |
| **30-40% model / 60-70% architecture**        | Stacked bar showing contribution split  | 18/26 sources    |

### Validation Metrics (Credibility Footer)

| Metric                                | Visual Treatment             |
| ------------------------------------- | ---------------------------- |
| **26 sources analyzed**               | Source type breakdown icons  |
| **92% mentioned integration**         | Theme frequency indicator    |
| **90% enterprise pilot failure rate** | Failure/success ratio visual |

---

## Visual Concept

**Dashboard-Style Infographic with Data Cards**

A clean, modern dashboard layout that presents each metric as a distinct "data card" - similar to analytics dashboards or research report visualizations. Each card contains:

- The key number (large, bold)
- A simple visualization reinforcing the data
- Source frequency indicator (showing research credibility)
- Brief explanatory label

The layout creates a visual hierarchy from most impactful (top) to supporting evidence (bottom), guiding the reader through the research findings systematically.

---

## Elements

### Header Section

- **Title**: "The Numbers Behind Production AI Agents"
- **Subtitle**: "What 36 Expert Interviews Revealed About Deployment Reality"
- **Research badge**: "GSBGEN 390 | Stanford GSB | n=26 sources"

### Row 1: Time & Integration (Hero Section)

**Card 1A: Integration Time**

- Giant number: "40-50%"
- Sub-label: "of deployment time spent on integration"
- Visual: Circular donut chart with integration segment highlighted
- Source indicator: Small bar showing 15/26 sources
- Icon: Puzzle pieces connecting

**Card 1B: Context Window Rule**

- Giant number: "40%"
- Sub-label: "max context utilization before degradation"
- Visual: Memory buffer bar filled to 40%, with caution zone beyond
- Source indicator: Small bar showing 8/26 sources
- Icon: Document stack with limit marker

**Card 1C: Tool Accuracy Threshold**

- Giant number: "25"
- Sub-label: "tools = MCP accuracy cliff"
- Visual: Ascending line that drops sharply after 25 tools
- Source indicator: Small bar showing 6/26 sources
- Icon: Toolbox with numbered slots

### Row 2: Evaluation Insights (Secondary Section)

**Card 2A: Evaluation Approach**

- Visual: Two-panel comparison
- Left panel (dimmed): "End-to-End Testing" with X mark
- Right panel (highlighted): "Component-Level Eval" with checkmark
- Label: "Component testing outperforms holistic evaluation"
- Tag: "Emergent Finding"

**Card 2B: Success Metric**

- Visual: Agent-to-human handoff diagram
- Primary text: "Handoff Rate"
- Secondary text: "Not technical sophistication"
- Label: "The better success metric"
- Tag: "Emergent Finding"

### Row 3: The Core Insight (Context Strip)

**Full-width card: Contribution Split**

- Visual: Horizontal stacked bar
- Left segment (30-40%): "Model Capabilities" - lighter shade
- Right segment (60-70%): "Architecture & Engineering" - darker, emphasized
- Label: "What actually drives production success"
- Source indicator: 18/26 sources

### Footer Section: Research Credibility

**Credibility Bar**

- Left: "26 Sources" with breakdown (17 interviews + 5 conferences + 3 prototypes + 1 confidential)
- Center: "92% cited integration as top challenge"
- Right: "90% pilot failure rate" with visual indicator
- Bottom: Stanford GSB attribution

---

## Color Scheme

### Primary Colors

- **Deep Navy**: #1a365d - Primary text, headers, emphasis areas
- **Ocean Blue**: #2c5282 - Secondary backgrounds, supporting elements
- **Ice Blue**: #63b3ed - Highlights, success indicators, key numbers

### Accent Colors

- **Warning Orange**: #ed8936 - Threshold indicators, caution zones
- **Coral Red**: #fc8181 - Failure indicators, problem areas
- **Success Green**: #68d391 - Positive outcomes, checkmarks

### Neutral Colors

- **Light Gray**: #f7fafc - Card backgrounds, light sections
- **Medium Gray**: #a0aec0 - Labels, secondary text
- **Dark Gray**: #4a5568 - Body text

### Usage Guidelines

- Navy for authoritative/primary content
- Orange for thresholds and limits (40% context, 25 tools)
- Blue gradient for data visualizations
- White cards with subtle shadows for readability
- Gray for de-emphasized supporting information

---

## Dimensions

### Full Infographic

- **Primary**: 800x1200px (portrait orientation for full data story)
- **Print-ready**: 1600x2400px (2x for retina)

### Social Variants

- **LinkedIn/Twitter**: 1200x630px (horizontal excerpt with top 3 metrics)
- **Instagram Square**: 1080x1080px (single metric focus per slide)
- **Story Format**: 1080x1920px (vertical scroll version)

### Modular Cards

- Individual metric cards: 380x280px each
- Allows for carousel/slideshow use

---

## Typography

### Font Family

- **Primary**: Inter or SF Pro Display
- **Numbers**: Tabular figures enabled for data alignment
- **Fallback**: -apple-system, system-ui, sans-serif

### Type Scale

- **Hero Numbers**: 72px bold (key metrics like "40-50%")
- **Card Titles**: 24px semibold
- **Labels**: 16px medium
- **Source Indicators**: 12px regular
- **Fine Print**: 10px light

---

## Style Notes

### Design Principles

- **Data-forward**: Numbers are the heroes; visuals support, not distract
- **Clean and scannable**: White space between cards, clear hierarchy
- **Research credibility signals**: Source frequency on every metric
- **Consistent visual language**: Same card structure throughout
- **No decorative elements**: Every pixel serves information purpose

### Visual Grammar

- Rounded corners (8px radius) on all cards
- Subtle drop shadows (0 2px 4px rgba(0,0,0,0.1))
- 16px grid system for alignment
- 24px minimum spacing between elements

### What to Avoid

- Cluttered visualizations with too many data points
- 3D effects or gradients that obscure data
- Stock imagery or generic AI visuals
- Overly complex charts requiring explanation
- Dense text blocks - let numbers speak

### Inspiration References

- Stripe's API documentation visualizations
- Our World in Data infographics
- Financial Times data journalism
- Anthropic and OpenAI research paper figures
- McKinsey/BCG research report graphics

---

## Production Notes

### File Formats

- Master: Figma/Sketch source file
- Export: PNG (transparent background option)
- Social: JPG (optimized, <300KB)
- PDF: Vector version for presentations

### Accessibility

- Minimum contrast ratio 4.5:1 for all text
- Color not sole indicator of meaning
- Alt text prepared for each metric card
- Screen-reader-friendly number formatting

### Version Control

- v1: Full infographic with all metrics
- v2: Social media horizontal excerpt
- v3: Individual metric cards for carousel
- v4: Presentation slide version (16:9)

---

## Metric Prioritization Rationale

### Tier 1 (Hero Placement)

1. **40-50% integration time** - Most universally cited (15/26), directly challenges "AI is the hard part" assumption
2. **40% context window rule** - Concrete, actionable threshold with clear number
3. **25-tool MCP threshold** - Surprising specificity, highly shareable

### Tier 2 (Supporting Evidence)

4. **Component vs. End-to-End eval** - Actionable insight for practitioners
5. **Handoff rate metric** - Paradigm shift in how to measure success

### Tier 3 (Credibility Context)

6. **30-40% model contribution** - Core research finding, establishes framework
7. **Source frequency data** - Research methodology transparency
8. **90% failure rate** - Stakes/urgency for the reader

---

_Specification created: December 3, 2025_
_For: AI Agents Research Publishing Project_
_Reference: ProcessRecap.md, 26 source extractions_
