# Homepage Featured Posts

## Primary Featured Post (Hero)

**Title:** What's Really Blocking AI Agents from Production? Insights from 36 Expert Interviews
**File:** 2025-12-ai-agents-research-overview.mdx
**Rationale:** Anchor publication provides comprehensive overview of the entire research project. This post synthesizes all 36 expert interviews, 5 industry conferences, and 3 functional prototypes into the core thesis that AI agent deployment is fundamentally an engineering problem (60-70%) rather than an AI problem (30-40%). Features the most compelling statistics including 92% system integration challenge and 90% pilot failure rate. Essential starting point for readers entering the research series.

---

## Secondary Featured Posts (Grid of 3-4)

| Priority | Title                                                                                           | File                                      | Rationale                                                                                                                                                                                                                                                                                                      |
| -------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1        | The 92% Problem: Why System Integration Breaks AI Agent Deployments                             | 2025-12-system-integration-92-percent.mdx | Highest-impact finding from the research with the most compelling statistic. Directly challenges the assumption that AI capability is the bottleneck and provides actionable insights about the 40-50% integration time allocation. MCP limitations (25-tool cliff) are immediately relevant to practitioners. |
| 2        | Why 95% of Agentic AI Projects Fail: Key Takeaways from the Conference Everyone's Talking About | 2025-12-why-95-fail.mdx                   | Introduces the critical "handoff rate" metric that reframes agent success measurement. The four traps taxonomy (RAG chatbot, drag-and-drop platform, MCP context bloat, tech debt) provides immediate value for teams planning AI agent projects. Highly shareable title with clear takeaways.                 |
| 3        | The Coding Agent Exception: Why AI Works for Code but Struggles Everywhere Else                 | 2025-12-coding-agent-exception.mdx        | Provides essential context for the generation vs. analysis asymmetry that explains why coding agents succeed (3x productivity) while generic agents fail (3-5x cost disadvantage). Counterintuitive finding that challenges broad AI automation narratives while validating specific use cases.                |
| 4        | The Demo-Production Chasm: Why 70% Accuracy Creates False Expectations                          | 2025-12-demo-production-chasm.mdx         | Addresses the expectation management challenge that underlies the 90% pilot failure rate. The "doom loop" concept is immediately recognizable to practitioners. Provides architectural patterns (state machines, verification phases, risk-based HITL) that bridge demo success to production reliability.     |

---

## Rotation Schedule

### Week 1-2: Launch Configuration (Above)

- **Hero:** Research Overview (anchor post)
- **Grid:** System Integration, Why 95% Fail, Coding Agent Exception, Demo-Production Chasm
- **Rationale:** Establish research credibility with comprehensive overview and highest-impact findings

### Week 3-4: Rotate to Practitioner Perspectives

- **Hero:** an AI autonomous agent company Fireside: The 30-40% Model Revelation That Changed Everything
- **Grid:**
  - The 40% Rule: Why Larger Context Windows Won't Save Your AI Agent
  - The Framework Paradox: Why 80-90% of Teams Abandon a popular AI agent framework for Production
  - Handoff Rate: The North Star Metric for AI Agent Success
  - The 30-40% Model Myth: Why Framework Architecture Matters More Than Model Capability
- **Rationale:** Shift focus to actionable insights and technical patterns for practitioners actively building

### Week 5-6: Feature Prototype Learnings

- **Hero:** Research Overview (return to anchor)
- **Grid:**
  - Interview posts (an AI agent orchestration company, an AI infrastructure company, a major enterprise identity company, a multi-agent framework company)
  - Prototype posts (Shopping Agent, Repo Patcher, Good Agents)
- **Rationale:** Surface depth content for engaged readers who have consumed headline findings

### Week 7+: Ongoing Rotation Based on Engagement

- Rotate based on analytics data
- Feature any posts with high engagement or social sharing
- Refresh hero periodically to maintain site freshness

---

## Featured Post Criteria

1. **High Relevance Score** - Posts that address the most frequently cited challenges across the 36 interviews and 5 conferences
2. **Engaging Title** - Posts with clear, specific statistics or counterintuitive framings that invite clicks
3. **Key Research Findings** - Posts containing novel insights not widely discussed elsewhere (30-40% model contribution, 92% integration, 40% context rule)
4. **Actionable Insights** - Posts that provide concrete recommendations practitioners can implement immediately

---

## Top 4 Picks Summary

### 1. Research Overview (Hero)

The comprehensive anchor that establishes the research's credibility and central thesis. Optimal entry point for all readers.

### 2. System Integration (92%)

The most compelling statistic from the research, immediately challenges conventional wisdom, high relevance for technical decision-makers.

### 3. Why 95% Fail

The handoff rate metric is the single most actionable takeaway; the four traps provide a diagnostic framework for any AI agent project.

### 4. Coding Agent Exception

Critical for expectation-setting; explains the generation vs. analysis asymmetry that determines which AI investments will succeed today.

---

## Implementation Notes

- All featured posts should display the author (Fernando Torres) and Stanford GSB affiliation prominently
- Include "Part of a 25-post research series" badge on all featured items
- Link to /blog/ai-agents-research as the canonical series landing page
- Consider adding reading time estimates (most posts are 8-12 minute reads)
- Hero post should display the key statistics prominently: 36 interviews, 5 conferences, 3 prototypes
